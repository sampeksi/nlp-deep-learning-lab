{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Lecture 4 Example: Retrieval Augmented Generation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LLMs locally, download it from here:\n",
    "\n",
    "https://ollama.com/\n",
    "\n",
    "Then you can pull LLMs models by pull it from your terminal:\n",
    "\n",
    "`ollama pull mistral`\n",
    "\n",
    "And to run it:\n",
    "\n",
    "`ollama run mistral`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGs pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](rag_pipeline.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents and DataBase Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the boardgame rules as the documents for our RAGs system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chromadb\n",
    "%pip install ollama\n",
    "%pip install wikipedia\n",
    "%pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import chromadb\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chroma and data path\n",
    "chroma_path = 'chroma'\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "def load_wikipedia_articles(topic: str):\n",
    "    \"\"\"Load Wikipedia articles based on a given topic.\"\"\"\n",
    "    loader = WikipediaLoader(query=topic, lang=\"en\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} Wikipedia articles on '{topic}'\")\n",
    "    return documents\n",
    "\n",
    "wikipedia_docs = load_wikipedia_articles(\"Retrieval-Augmented Generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"Chunk Wikipedia documents into smaller pieces for embedding.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,  # Size of each chunk\n",
    "        chunk_overlap=80,  # Overlapping words between chunks\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Split Wikipedia content into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Split the Wikipedia documents into chunks\n",
    "chunks = split_documents(wikipedia_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    chuck_ids = [f\"id{x}\" for x in range(len(chunks))]\n",
    "    return chuck_ids\n",
    "\n",
    "print(calculate_chunk_ids(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the `OllamaEmbeddings` model from the `langchain_community` library to embed our documents. This model will help us convert the text data into numerical vectors, which can be used for various downstream tasks such as similarity search, clustering, and more.\n",
    "\n",
    "The `OllamaEmbeddings` model is initialized with the `nomic-embed-text` model, which is specifically designed for embedding text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "def embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model='nomic-embed-text')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add chunking documents to the Chroma DB using the `OllamaEmbeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks: list):\n",
    "    \"\"\"Adds Wikipedia chunks to ChromaDB while checking for duplicates.\"\"\"\n",
    "    # Initialize ChromaDB\n",
    "    db = Chroma(\n",
    "        persist_directory=chroma_path, \n",
    "        embedding_function=embedding_function()\n",
    "    )\n",
    "\n",
    "    # Generate unique IDs for chunks\n",
    "    chunk_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Fetch existing document IDs\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Filter out chunks that are already in the database\n",
    "    new_chunks = [chunk for i, chunk in enumerate(chunks) if chunk_ids[i] not in existing_ids]\n",
    "\n",
    "    if new_chunks:\n",
    "        print(f\"Adding {len(new_chunks)} new documents...\")\n",
    "        new_chunk_ids = [chunk_ids[i] for i in range(len(new_chunks))]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "        print(\"Successfully added new documents to ChromaDB!\")\n",
    "    else:\n",
    "        print(\"No new documents to add.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconnect to the existing ChromaDB instance\n",
    "def load_chroma():\n",
    "    \"\"\"Load the existing ChromaDB with stored documents.\"\"\"\n",
    "    return Chroma(\n",
    "        persist_directory=chroma_path,  # Load from saved DB\n",
    "        embedding_function=embedding_function()\n",
    "    )\n",
    "\n",
    "# Load ChromaDB and check how many documents exist\n",
    "db = load_chroma()\n",
    "existing_items = db.get(include=[\"documents\"])  # FIXED LINE\n",
    "print(f\"Total documents in ChromaDB: {len(existing_items['documents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will query the data using the RAGs system. We will use the `OllamaEmbeddings` model to embed the query text and search the Chroma database for relevant documents. The results will be formatted and displayed along with their sources.\n",
    "\n",
    "The following steps will be performed:\n",
    "1. Prepare the Chroma database with the embedding function.\n",
    "2. Search the database for the most similar documents to the query text.\n",
    "3. Format the results and display the response along with the sources.\n",
    "\n",
    "The `query_rag` function will handle these steps and return the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "chroma_path = 'chroma'\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str):\n",
    "    \"\"\"Query the RAG system with a user question.\"\"\"\n",
    "    \n",
    "    # Load the existing ChromaDB\n",
    "    embedding = embedding_function()\n",
    "    db = Chroma(persist_directory=chroma_path, embedding_function=embedding)\n",
    "\n",
    "    # Perform similarity search to retrieve relevant chunks\n",
    "    results = db.similarity_search_with_score(query_text, k=3)  # Top 3 relevant docs\n",
    "\n",
    "    if not results:\n",
    "        print(\"No relevant documents found!\")\n",
    "        return\n",
    "    \n",
    "    # Combine the results into a single context string\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    # Format the prompt with the retrieved context\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    \n",
    "    print(f\"Querying Ollama with prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    # Query the Ollama model\n",
    "    model = Ollama(model=\"mistral\")  # Ensure Ollama is running\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    # Extract source document IDs\n",
    "    sources = [doc.metadata.get(\"id\", \"Unknown\") for doc, _score in results]\n",
    "\n",
    "    # Display results\n",
    "    print(\"Response:\\n\", response_text)\n",
    "    print(\"\\nSources:\", sources)\n",
    "\n",
    "    return response_text, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rag(\"How does Retrieval-Augmented Generation work?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
