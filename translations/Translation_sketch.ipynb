{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model and Attention Mechanisms\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset using the `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('bentrevett/multi30k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train dataset into train, validation, and test sets\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data, test_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens and parameters\n",
    "max_length = 1000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "min_freq = 2\n",
    "special_tokens = [unk_token, pad_token, sos_token, eos_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install tokenizer for English and German:\n",
    "\n",
    "`python -m spacy download en_core_web_sm`\n",
    "\n",
    "`python -m spacy download de_core_news_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/python3 -m spacy download en_core_web_sm\n",
    "!/usr/bin/python3 -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "de_nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence, en_vocab, de_vocab):\n",
    "    # Tokenize English and German\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(sentence[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(sentence[\"de\"])][:max_length]  \n",
    "\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "\n",
    "    # Add special tokens\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "\n",
    "    # Numericalize tokens\n",
    "    en_ids = [en_vocab.get(token, en_vocab[unk_token]) for token in en_tokens]\n",
    "    de_ids = [de_vocab.get(token, de_vocab[unk_token]) for token in de_tokens]\n",
    "\n",
    "    return {\n",
    "        \"en\": sentence[\"en\"],\n",
    "        \"de\": sentence[\"de\"],\n",
    "        \"en_tokens\": en_tokens,\n",
    "        \"de_tokens\": de_tokens,\n",
    "        \"en_ids\": en_ids,\n",
    "        \"de_ids\": de_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build Vocabulary\n",
    "def build_vocab(data, min_freq, specials):\n",
    "    counter = Counter()\n",
    "    for tokens in data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {token: idx for idx, token in enumerate(specials)}\n",
    "    idx = len(vocab)\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq and token not in vocab:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate Tokenized Data for Vocabulary Building\n",
    "tokenized_train_data = train_data.map(\n",
    "    lambda example: {\"en_tokens\": [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length],\n",
    "                     \"de_tokens\": [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "en_vocab = build_vocab(tokenized_train_data[\"en_tokens\"], min_freq, special_tokens)\n",
    "de_vocab = build_vocab(tokenized_train_data[\"de_tokens\"], min_freq, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Process Full Dataset\n",
    "train_data = train_data.map(lambda x: process_sentence(x, en_vocab, de_vocab))\n",
    "valid_data = valid_data.map(lambda x: process_sentence(x, en_vocab, de_vocab))\n",
    "test_data = test_data.map(lambda x: process_sentence(x, en_vocab, de_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabularies to JSON\n",
    "with open(\"en_vocab.json\", \"w\") as f:\n",
    "    json.dump(en_vocab, f)\n",
    "with open(\"de_vocab.json\", \"w\") as f:\n",
    "    json.dump(de_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for special tokens in both vocabularies - this is debugging\n",
    "assert en_vocab[unk_token] == de_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == de_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "pprint(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "\n",
    "        # Pad sequences along the first dimension to maintain [seq_length, batch_size]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, batch_first=False, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, batch_first=False, padding_value=pad_index)\n",
    "\n",
    "        # Create a batch dictionary\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids, \n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Encoder and Decoder for Seq2Seq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder reads the input sequence and summerizes the information in something called internal state vectors or context vectors. This context vector aims to encapsulate the information for all input elements to help the decoder make accurate predictions.\n",
    "- This implementation involves creating an RNN-based encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, define class Encoder with __init__ and forward function\n",
    "# collate_fn method edited due to inconsistent batch sizes\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        # Pass through embedding layer\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        # Pack padded sequences for efficiency\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # Unpack sequences\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "\n",
    "        # Return outputs, hidden state, and cell state\n",
    "        return outputs, hidden, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, introduce class Decoder here\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to map hidden state to output vocabulary\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, hidden, cell):\n",
    "        # Take only the last token (teacher forcing step-by-step)\n",
    "        tgt = tgt.unsqueeze(1)\n",
    "\n",
    "        # Pass through embedding layer\n",
    "        embedded = self.dropout(self.embedding(tgt))\n",
    "\n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "\n",
    "        # Return prediction, hidden state, and cell state\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final part of the implemenetation, we'll implement the seq2seq model. This will handle:\n",
    "\n",
    "- receiving the input/source sentence\n",
    "- using the encoder to produce the context vectors\n",
    "- using the decoder to produce the predicted output/target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, introduce a Seq2Seq class using the encoder and the decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = tgt.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "        # Encode the source sequence\n",
    "        src_lengths = (src != 0).sum(dim=1)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        # Initialize the decoder input with the <sos> token\n",
    "        input = tgt[:, 0]\n",
    "\n",
    "        # Iterate through the target sequence\n",
    "        for t in range(1, tgt_len):\n",
    "            # Pass through the decoder\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # Store the prediction\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "\n",
    "            # Get the highest predicted token\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # Use teacher forcing or prediction as the next input\n",
    "            input = tgt[:, t] if teacher_force and t + 1 < tgt_len else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(de_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input Dim (German Vocab):\", input_dim)\n",
    "print(\"Output Dim (English Vocab):\", output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.01, 0.01)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert encoder.hidden_dim == decoder.hidden_dim\n",
    "assert encoder.n_layers == decoder.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"de_ids\"].to(device)\n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"de_ids\"].to(device)\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_loader,\n",
    "        criterion,\n",
    "\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tranlate-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"tranlate-model.pt\"))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_loader, criterion)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_length=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = [token.text.lower() for token in de_nlp(sentence)]\n",
    "    # Add <sos> and <eos> tokens\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    # Convert tokens to indices\n",
    "    src_indexes = [src_vocab.get(token, src_vocab[unk_token]) for token in tokens]\n",
    "    # Convert to tensor and add batch dimension\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Encode the source sentence\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "    \n",
    "    # Initialize the target sentence with <sos> token\n",
    "    trg_indexes = [trg_vocab[sos_token]]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        if pred_token == trg_vocab[eos_token]:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on test data\n",
    "sentence = test_data[0][\"de\"]\n",
    "expected_result = test_data[0][\"en\"]\n",
    "translation = translate_sentence(sentence, de_vocab, en_vocab, model, device)\n",
    "\n",
    "print(\"Translated sentence:\", \" \".join(translation))\n",
    "print(\"expected_result:\", expected_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for a single reference and candidate sentence pair.\n",
    "    \n",
    "        :param reference: List of words in the target sentence (ground truth).\n",
    "        :param candidate: List of words in the predicted sentence.\n",
    "\n",
    "    Return: BLEU score (float)\n",
    "    \"\"\"\n",
    "    return sentence_bleu([reference], candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = calculate_bleu_score(expected_result, translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
