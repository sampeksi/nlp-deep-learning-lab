{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install numpy\n",
    "%pip install python-utils\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import twitter_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Traditional Sentiment Analysis approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocessing the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk samples, stopwords\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Read corpus package\n",
    "print(twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Process tweet function.\n",
    "    \n",
    "    Input:\n",
    "        tweet: a string containing a tweet.\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet.\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract positive and negative tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "\n",
    "all_tweets = positive_tweets + negative_tweets\n",
    "\n",
    "# Create labels: 1 for positive, 0 for negative\n",
    "positive_labels = [1] * len(positive_tweets)\n",
    "negative_labels = [0] * len(negative_tweets)\n",
    "\n",
    "all_labels = positive_labels + negative_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emotion data set. Data set should be located to same the path as jupyter notebook\n",
    "\n",
    "emotion_data = pd.read_csv('combined_emotion.csv')\n",
    "\n",
    "positive_emotions = emotion_data[emotion_data['emotion'] == 'joy']\n",
    "negative_emotions = emotion_data[emotion_data['emotion'] == 'sad']\n",
    "\n",
    "all_emotions = pd.concat([positive_emotions, negative_emotions])\n",
    "print(all_emotions.head())\n",
    "\n",
    "\n",
    "pos_emotion_labels = [1] * len(positive_emotions)\n",
    "neg_emotion_labels = [0] * len(negative_emotions)\n",
    "\n",
    "all_emotion_labels = pos_emotion_labels + neg_emotion_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataFrame for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(tweets, labels):\n",
    "    \"\"\"\n",
    "    Create DataFrame to visualize the dataset.\n",
    "    \n",
    "    Input:\n",
    "        tweet: a list containing tweet texts.\n",
    "        labels: list containing labels for each tweet (1 for positve, 0 for negative).\n",
    "    Output:\n",
    "        A DataFrame with 2 columns of tweets and labels.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(tweets) != len(labels):\n",
    "        raise ValueError\n",
    "\n",
    "    df = pd.DataFrame({'tweets': tweets, 'labels': labels})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for tweets and labels\n",
    "df = create_dataframe(all_tweets, all_labels)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = pd.DataFrame({'sentence': all_emotions['sentence'].tolist(), 'label': all_emotion_labels})\n",
    "print(emotion_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for the Word2Vec model\n",
    "cleaned_tweets = []\n",
    "for tweet in all_tweets:\n",
    "    cleaned_tweet = process_tweet(tweet)\n",
    "    cleaned_tweets.append(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "# May take over a 1min\n",
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_text(text):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            stem_word = stemmer.stem(word)  # Apply stemming\n",
    "            cleaned_tokens.append(stem_word)\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Apply preprocessing to emotion data\n",
    "emotion_df['cleaned_text'] = emotion_df['sentence'].apply(process_text)\n",
    "cleaned_emotions = emotion_df['cleaned_text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings using Word2Vec model\n",
    "word2vec_model = Word2Vec(cleaned_tweets, vector_size=20,\n",
    "                          window=5, min_count=5, workers=4)\n",
    "\n",
    "word_embeddings = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_emotion_model = Word2Vec(cleaned_emotions, vector_size=20,\n",
    "                          window=5, min_count=5, workers=4)\n",
    "\n",
    "emotion_embeddings = word2vec_emotion_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using word embeddings\n",
    "print(word_embeddings['listen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotion_embeddings['listen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tweet embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each tweet consists of multiple words, convert each tweet into a single vector representation by averaging the Word2Vec embeddings of all the words in the tweet. This averaged vector will represent the tweet in a fixed-dimensional space, suitable for input into a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(tweet_tokens, word2vec_model):\n",
    "    \"\"\"\n",
    "    Generate the embedding for a tweet by averaging word vectors.\n",
    "    \n",
    "    Input: \n",
    "        tweet_tokens: a list of tokens from processed tweet.\n",
    "        word2vec_model: a trained Word2Vec model that contains word embeddings.\n",
    "    Output:\n",
    "        tweet_embedding: a numpy array representing the averaged embedding \n",
    "                        vector for a tweet. The dimension of the array is \n",
    "                        equal to the vector_size of the Word2Vec model.\n",
    "\n",
    "    \"\"\"\n",
    "    tweet_vecs = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if word in word2vec_model.wv:\n",
    "            tweet_vecs.append(word2vec_model.wv[word])\n",
    "\n",
    "    if len(tweet_vecs) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "    tweet_embedding = np.mean(tweet_vecs, axis=0)\n",
    "\n",
    "    return tweet_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using get_tweet_embeddings function\n",
    "tweet1 = df['tweets'][4]\n",
    "tweet1_cleaned = process_tweet(tweet1)\n",
    "print(tweet1_cleaned)\n",
    "tweet1_embedding = get_embedding(tweet1, word2vec_model)\n",
    "print(tweet1_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to a numpy array\n",
    "labels = np.array(all_labels)\n",
    "\n",
    "# Generate embeddings for all tweets in the dataset\n",
    "tweet_embeddings = np.array([get_embedding(\n",
    "    tweet, word2vec_model) for tweet in cleaned_tweets])\n",
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    tweet_embeddings, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to a numpy array\n",
    "emotion_labels = np.array(all_emotion_labels)\n",
    "\n",
    "# Generate embeddings for all emotions in the dataset\n",
    "emotion_embeddings = np.array([get_embedding(\n",
    "    emotion, word2vec_emotion_model) for emotion in cleaned_emotions])\n",
    "\n",
    "# Split the dataset into training and test sets (80% train, 20% test)\n",
    "x_em_train, x_em_test, y_em_train, y_em_test = train_test_split(\n",
    "    emotion_embeddings, emotion_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Classifier using Logistic Regression as a baseline classifier, which works well for binary classification tasks like sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the logistic regression classifier\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the logistic regression classifier\n",
    "em_clf = LogisticRegression(random_state=42)\n",
    "em_clf.fit(x_em_train, y_em_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_em_pred = clf.predict(x_em_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy: \", accuracy)\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "em_accuracy = accuracy_score(y_em_test, y_em_pred)\n",
    "print(\"Test Accuracy: \", em_accuracy)\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_em_test, y_em_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example prediction by using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, word2vec_model, clf):\n",
    "    \"\"\"\n",
    "    Predict sentiment of a tweet using a trained Word2Vec model and classifier.\n",
    "    \n",
    "    Input:\n",
    "        tweet: raw text to predict sentiment.\n",
    "        word2vec_model: a trained Word2Vec model containing word embeddings.\n",
    "        clf: a trained classifier for sentiment predictions.\n",
    "    Output:\n",
    "        Returns \"Positive\" if the predicted sentiment is positive, \n",
    "                otherwise returns \"Negative\".\n",
    "                \n",
    "    \"\"\"\n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    tweet_embedding = get_embedding(processed_tweet, word2vec_model)\n",
    "    prediction = clf.predict([tweet_embedding])\n",
    "\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(sentence, word2vec_emotion_model, em_clf):\n",
    "\n",
    "    processed_sentence = process_text(sentence)\n",
    "    sentence_embedding = get_embedding(processed_sentence, word2vec_emotion_model)\n",
    "    prediction = em_clf.predict([sentence_embedding])\n",
    "\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "new_tweet = \"I like to study NLP <3\"\n",
    "print(\"Sentiment: \", predict_tweet(new_tweet, word2vec_model, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"I like to study NLP <3\"\n",
    "print(\"Sentiment: \", predict_emotion(new_sentence, word2vec_emotion_model, em_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Layer Perceptrons (MLP) can enhance the predictive power of sentiment analysis model by allowing it to capture more complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline for building a Multi-Layer Perceptrons for Sentiment Analysis\n",
    "\n",
    "#### 1. Data Preparation: \n",
    "- Use the embeddings generated for each tweet as input features for the MLP.\n",
    "\n",
    "#### 2. Model Architecture:\n",
    "- Design a simple MLP with several fully connected layers (dense layers), an activation function (ReLu) for non-linearity, and dropout layers to prevent overfitting.\n",
    "\n",
    "- Use a final ouput layer with a sigmoid activation function for binary classification.\n",
    "\n",
    "#### 3. Training and Evaluation:\n",
    "- Train the MLP on the training set, validate on the test set, and evaluate performance using accuracy and a classification report.\n",
    "\n",
    "#### 4. Hyperparameter Tuning:\n",
    "- Experiment with the number of layers, number of neurons, dropout rates, and learning rate to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Multi-Layer Perceptron (MLP) class in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentMLP(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, \n",
    "                 hidden_dim1: int, \n",
    "                 hidden_dim2: int, \n",
    "                 dropout: float):\n",
    "        \"\"\"\n",
    "        Initialize the Multi-Layer Perceptrons (MLP) model for sentiment analysis.\n",
    "\n",
    "        Parameters:\n",
    "            input_dim (int): dimension of the input features (tweet embedding size).\n",
    "            hidden_dim1 (int): Number of neurons in the first hidden layer.\n",
    "            hidden_dim2 (int): Number of neurons in the second hidden layer.\n",
    "            dropout_rate (float): Dropout rate to prevent overfitting.\n",
    "\n",
    "        \"\"\"\n",
    "        super(SentimentMLP, self).__init__()\n",
    "\n",
    "        # Using nn.Sequential to stack layers\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim1),  # First hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),  # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim2, 1),  # Output layer\n",
    "            nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tweet_embeddings)  # tweet_embeddings generated from Word2Vec\n",
    "y = np.array(labels)  # labels for the tweets (1 for positive, 0 for negative)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_X = np.array(emotion_embeddings)\n",
    "em_y = np.array(emotion_labels)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "em_X = torch.tensor(em_X, dtype=torch.float32)\n",
    "em_y = torch.tensor(em_y, dtype=torch.float32)\n",
    "\n",
    "# Split the data into training and test datasets\n",
    "X_em_train, X_em_test, y_em_train, y_em_test = train_test_split(em_X, em_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters for the model\n",
    "embedding_dim = X_train.shape[1]\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "dropout_rate = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = SentimentMLP(\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim1=hidden_dim1,\n",
    "    hidden_dim2=hidden_dim2,\n",
    "    dropout=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_embedding_dim = X_em_train.shape[1]\n",
    "em_hidden_dim1 = 128\n",
    "em_hidden_dim2 = 64\n",
    "em_dropout_rate = 0.2\n",
    "\n",
    "em_mlp_model = SentimentMLP(\n",
    "    embedding_dim=em_embedding_dim,\n",
    "    hidden_dim1=em_hidden_dim1,\n",
    "    hidden_dim2=em_hidden_dim2,\n",
    "    dropout=em_dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, criterion, optimizer, num_epochs, batch_size, print_every):\n",
    "    \"\"\"\n",
    "    Train a model with the given dataset, loss function, and optimizer.\n",
    "\n",
    "    Parameters:\n",
    "        model: the neural network model to train.\n",
    "        x_train: training features.\n",
    "        y_train: training labels.\n",
    "        criterion: loss function.\n",
    "        optimizer: optimizer to update model parameters.\n",
    "        num_epochs: number of epochs to train.\n",
    "        batch_size: size of each batch to training.\n",
    "        print_every: frequency of printing loss (e.g, every 5 epochs)\n",
    "    \n",
    "    Returns:\n",
    "        A list of loss values for each epoch.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    epoch_losses = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        permutation = torch.randperm(x_train.size(0))\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(0, x_train.size(0), batch_size):\n",
    "            # Select mini-batch\n",
    "            index = permutation[i:i+ batch_size]\n",
    "            batch_x, batch_y = x_train[index], y_train[index]\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_x).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for each epoch\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Average loss for each epoch\n",
    "        avg_epoch_loss = epoch_loss / len(permutation)\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "\n",
    "        # Print progress for each epoch\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}, Loss: {avg_epoch_loss}]\")\n",
    "    \n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_criterion = nn.BCELoss()\n",
    "em_optimizer = optim.Adam(em_mlp_model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, losses = train(mlp_model, X_train, y_train, criterion, optimizer, num_epochs=20, batch_size=64, print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_model, em_losses = train(em_mlp_model, X_em_train, y_em_train, em_criterion, em_optimizer, num_epochs=30, batch_size=256, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset and print performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        model (nn.Module): Trained model to evaluate.\n",
    "        X_test (torch.Tensor): Test features.\n",
    "        y_test (torch.Tensor): Test labels.\n",
    "        \n",
    "    Returns:\n",
    "        Accuracy of the model on the test set.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_test)\n",
    "        predictions = (outputs > 0.5).int()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(em_model, X_em_test, y_em_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4:  Evaluate emotion-trained model on tweet data\n",
    "evaluate(em_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5:  Evaluate tweet-trained model on emotion data\n",
    "evaluate(model, X_em_test, y_em_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Sentiment for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing the new data using process tweet function and Word2Vec model defined above.\n",
    "    \n",
    "    \"\"\"\n",
    "    processed_tokens = process_tweet(text)\n",
    "    \n",
    "    tweet_embedding = get_embedding(processed_tokens, word2vec_model)\n",
    "    \n",
    "    # Convert to a tensor and reshape to match the model's expected input shape\n",
    "    tweet_embedding_tensor = torch.tensor(tweet_embedding, dtype=torch.float32).unsqueeze(0)\n",
    "    return tweet_embedding_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_emotion(text):\n",
    "    \"\"\"\n",
    "    Preprocessing the new data using process tweet function and Word2Vec model defined above.\n",
    "    \n",
    "    \"\"\"\n",
    "    processed_tokens = process_text(text)\n",
    "    \n",
    "    en_embedding = get_embedding(processed_tokens, word2vec_emotion_model)\n",
    "    \n",
    "    em_embedding_tensor = torch.tensor(en_embedding, dtype=torch.float32).unsqueeze(0)\n",
    "    return em_embedding_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, is_emotion):\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a given text using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "        model: the trained MLP model.\n",
    "        text: input text to analyze.\n",
    "    \n",
    "    Returns:\n",
    "        \"Positive\" if sentiment is positive, otherwise \"Negative\".\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the text and get the embedding\n",
    "    input_tensor = \"\"\n",
    "    if is_emotion:\n",
    "        input_tensor = preprocess_emotion(text)\n",
    "    else:\n",
    "        input_tensor = preprocess_text(text)\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)  # Model outputs probability due to Sigmoid activation\n",
    "    \n",
    "    # Interpret the output\n",
    "    prediction = (output.item() > 0.5)  # Threshold at 0.5 for binary classification\n",
    "    sentiment = \"Positive\" if prediction else \"Negative\"\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text to analyze\n",
    "new_text = \"Oh great, it's raining again!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the sentiment\n",
    "sentiment = predict_sentiment(model, new_text, is_emotion=False)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sentiment = predict_sentiment(em_model, new_text, is_emotion=True)\n",
    "print(em_sentiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
