{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOj79n8sfeda"
      },
      "outputs": [],
      "source": [
        "%pip install transformers\n",
        "%pip install datasets\n",
        "%pip install tokenizers\n",
        "%pip install torch\n",
        "%pip install tqdm\n",
        "%pip install requests\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd6bGWKyfl49"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import requests\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure NLTK is set up\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhdCLHyNkvX6"
      },
      "source": [
        "# Data fetching from project Gutenberg\n",
        "\n",
        "We first download the books from Project Gutenberg and clean them by:\n",
        "- Removing metadata and irrelevant text.\n",
        "- Splitting them into meaningful sentences.\n",
        "- Filtering out incomplete or irrelevant sentences.\n",
        "\n",
        "This processed data will later be used for **retrieval and question-answering**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9K_7Rc7Dl_R"
      },
      "outputs": [],
      "source": [
        "def is_valid_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Filters out irrelevant sentences:\n",
        "    - Short sentences (less than 3 characters)\n",
        "    - Sentences containing mostly Roman numerals or numbers\n",
        "    - Sentences that look like unstructured lists (multiple commas without a verb)\n",
        "    \"\"\"\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    # Ignore very short sentences\n",
        "    if len(sentence) < 3:\n",
        "        return False\n",
        "\n",
        "    # Remove sentences that are mostly Roman numerals or numbers\n",
        "    if re.fullmatch(r\"[IVXLCDM]+\\.?\", sentence, re.IGNORECASE) or re.fullmatch(r\"\\d+\", sentence):\n",
        "        return False\n",
        "\n",
        "    # Filter out lines that seem like lists or metadata (many commas but no verbs)\n",
        "    if len(re.findall(r\",\", sentence)) > 3 and not re.search(r\"\\b(is|was|are|were|has|had|does|do|did|be|will|shall)\\b\", sentence):\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpmAp5vlfqdo"
      },
      "outputs": [],
      "source": [
        "def clean_text(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    print(f\"Downloaded the book successfully from {url}!\")\n",
        "\n",
        "    # Extract the main content\n",
        "    text = response.text\n",
        "    print(\"Raw text length:\", len(text))\n",
        "\n",
        "    # Find the book content\n",
        "    start_match = re.search(r\"(?i)(?:CHAPTER I|INTRODUCTION|PREFACE)\", text)\n",
        "    end_match = re.search(r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK\", text)\n",
        "\n",
        "    if start_match and end_match:\n",
        "        clean_text = text[start_match.start():end_match.start()].strip()\n",
        "    else:\n",
        "        print(\"Warning: Could not locate book boundaries correctly! Using full text as fallback.\")\n",
        "        clean_text = text  # Fallback to using full text\n",
        "\n",
        "    # Remove unwanted formatting using regex\n",
        "    clean_text = re.sub(r\"\\[.*?\\]\", \"\", clean_text)  # Remove content inside square brackets\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()  # Normalize spaces\n",
        "\n",
        "    print(\"Cleaned text length:\", len(clean_text))\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = nltk.sent_tokenize(clean_text)\n",
        "\n",
        "    # Apply filtering\n",
        "    filtered_sentences = [s for s in sentences if is_valid_sentence(s)]\n",
        "    print(f\"Filtered number of sentences: {len(filtered_sentences)}\")\n",
        "\n",
        "    return filtered_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "913ZSn7hfspX"
      },
      "outputs": [],
      "source": [
        "# URLs\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/cache/epub/56640/pg56640.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/67813/pg67813.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/20772/pg20772.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/40190/pg40190.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/4924/pg4924.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/4525/pg4525.txt\",\n",
        "    \"https://www.gutenberg.org/cache/epub/40190/pg40190.txt\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmTqjxOSk8fE"
      },
      "source": [
        "### Convert loaded data into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NcW-Fxjfum9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_dataset(urls):\n",
        "    dataset = []\n",
        "    book_id = 1  # Assign each book a unique ID\n",
        "\n",
        "    for url in urls:\n",
        "        sentences = clean_text(url)  # Get cleaned sentences\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            dataset.append({\n",
        "                \"book_id\": book_id,\n",
        "                \"text\": sentence\n",
        "            })\n",
        "\n",
        "        book_id += 1  # Increment book ID for the next book\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(\"ebook_dataset.csv\", index=False)\n",
        "    print(\"Dataset saved as 'ebook_dataset.csv'\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haU5314Gfw04"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "df = create_dataset(urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg5oie0-fzbV"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFs2fZE5KgSV"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3abBhzWmGLo"
      },
      "source": [
        "# **DPR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_9cxHCCBV6i"
      },
      "outputs": [],
      "source": [
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "import torch\n",
        "import faiss\n",
        "\n",
        "# Load the tokenizer for token-based chunking\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31TQkRTemNVi"
      },
      "source": [
        "### Chunk Text for Efficient Retrieval\n",
        "\n",
        "Since DPR works with **fixed-length input sequences**, we split the book text into **overlapping chunks**.\n",
        "\n",
        "- **Chunk Size:** 512 tokens (for compatibility with DPR).\n",
        "- **Stride:** 100 tokens (ensuring context overlap).\n",
        "- **Reason:** Improves retrieval accuracy by preserving sentence context.\n",
        "\n",
        "This allows the retriever to match queries with the most relevant passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIA_5zp4f4NV"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_tokens=512, stride=100):\n",
        "    \"\"\"\n",
        "    Splits text into overlapping chunks ensuring they respect token limits.\n",
        "    Uses a sliding window approach to preserve context.\n",
        "    \"\"\"\n",
        "    tokens = ctx_tokenizer(text, truncation=False)[\"input_ids\"]  # Tokenize entire text\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(tokens), max_tokens - stride):  # Overlapping chunks\n",
        "        chunk = tokens[i : i + max_tokens]  # Extract chunk\n",
        "        chunk_text = ctx_tokenizer.decode(chunk, skip_special_tokens=True)  # Convert back to text\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYuyN8ddf6_S"
      },
      "outputs": [],
      "source": [
        "# Convert Books into Chunks\n",
        "passages = []\n",
        "for book_id in df[\"book_id\"].unique():\n",
        "    book_text = \" \".join(df[df[\"book_id\"] == book_id][\"text\"].tolist())  # Merge all text from a book\n",
        "    text_chunks = chunk_text(book_text, max_tokens=512, stride=100)\n",
        "\n",
        "    for chunk in text_chunks:\n",
        "        passages.append({\"book_id\": book_id, \"text\": chunk})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXHN0EVbf87r"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame for easy use\n",
        "passage_df = pd.DataFrame(passages)\n",
        "print(\"Passages Created:\", passage_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHo4mz8CAFsK"
      },
      "outputs": [],
      "source": [
        "# Save for later use\n",
        "passage_df.to_csv(\"chunked_passages.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdZMCpsVmfpP"
      },
      "source": [
        "### Encode Passages Using DPR Context Encoder\n",
        "\n",
        "To enable fast and efficient retrieval, we encode book passages using the **DPR Context Encoder**.\n",
        "\n",
        "- **Model Used:** `facebook/dpr-ctx_encoder-single-nq-base`\n",
        "- **Purpose:** Converts each passage into a dense vector representation.\n",
        "\n",
        "These embeddings will be stored in a **FAISS index**, allowing fast similarity searches for question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa7yKWbdMFx9"
      },
      "outputs": [],
      "source": [
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc8nhxZFH3Gy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_passages(passages, batch_size=8):\n",
        "    \"\"\"Encodes passages using DPR Context Encoder in smaller batches to avoid OOM errors.\"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(passages), batch_size):\n",
        "        batch = passages[i : i + batch_size]  # Take a small batch\n",
        "\n",
        "        inputs = ctx_tokenizer(\n",
        "            batch,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embeddings = ctx_encoder(**inputs).pooler_output.cpu().numpy()  # Move to CPU to save GPU memory\n",
        "\n",
        "        all_embeddings.append(embeddings)\n",
        "        torch.cuda.empty_cache()  # Free memory after each batch\n",
        "\n",
        "    return np.vstack(all_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbQDOWMIBtMT"
      },
      "outputs": [],
      "source": [
        "# Encode all book passages\n",
        "print(\"Encoding passages for retrieval...\")\n",
        "passage_texts = passage_df[\"text\"].tolist()\n",
        "passage_embeddings = encode_passages(passage_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eozwKQirmmSK"
      },
      "source": [
        "### Build FAISS Index for Efficient Search\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) is used to store and retrieve **DPR passage embeddings**.\n",
        "\n",
        "- **Index Type:** `IndexFlatIP` (Inner Product similarity for cosine-based retrieval).\n",
        "- **Function:** Enables efficient **nearest neighbor search** to retrieve the best passage for a query.\n",
        "\n",
        "This significantly improves the retrieval speed when performing **question-answering or summarization**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtericqPBvoZ"
      },
      "outputs": [],
      "source": [
        "# Create FAISS Index\n",
        "print(\"Building FAISS index...\")\n",
        "faiss_index = faiss.IndexFlatIP(passage_embeddings.shape[1])  # Inner Product similarity\n",
        "faiss_index.add(passage_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpoZeSSvAssY"
      },
      "outputs": [],
      "source": [
        "# Load DPR Question Encoder (for query encoding)\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHAYEK5pH74D"
      },
      "outputs": [],
      "source": [
        "def encode_query(query):\n",
        "    \"\"\"Encodes the query using DPR Question Encoder with memory optimization.\"\"\"\n",
        "    inputs = q_tokenizer(\n",
        "        query,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embedding = q_encoder(**inputs).pooler_output.cpu().numpy()\n",
        "\n",
        "    torch.cuda.empty_cache()  # Free GPU memory\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj5wWI_SnB0m"
      },
      "source": [
        "### Retrieve Relevant Text from FAISS Index\n",
        "\n",
        "Once a **query embedding** is generated, we search for the **most relevant passage** using FAISS.\n",
        "\n",
        "- **Process:**\n",
        "  1. Compute **similarity scores** between query and stored passage embeddings.\n",
        "  2. Select the **top-k most relevant passages**.\n",
        "  3. Return the passage(s) that best answer the question.\n",
        "\n",
        "This step enables **context-aware question-answering and summarization**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1BqtE_iJkAJ"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_text(query, passage_df, top_k=3):\n",
        "    \"\"\"Finds the most relevant passage using FAISS nearest neighbor search.\"\"\"\n",
        "    query_embedding = encode_query(query)\n",
        "\n",
        "    # Search FAISS index\n",
        "    D, I = faiss_index.search(query_embedding, top_k)  # D = distances, I = indices\n",
        "\n",
        "    best_passages = [passage_df.iloc[idx][\"text\"] for idx in I[0]]\n",
        "\n",
        "    # âœ… Return multiple passages to improve answer extraction\n",
        "    return best_passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvqJpW0Uf_Ki"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsx1bE43vQQN"
      },
      "source": [
        "# Generating Questions Using T5\n",
        "\n",
        "To improve **Question Answering (QA) performance**, we **automatically generate questions** from book passages using **T5 (Text-To-Text Transfer Transformer)**.\n",
        "\n",
        "### **Why Use T5 for Question Generation?**\n",
        "- **Manually creating training questions is slow & inefficient.**\n",
        "- **T5 can automatically generate relevant questions from text passages.**\n",
        "- Helps fine-tune **DPR** and **BERT-QA** for better retrieval and answering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWcJ1pHFD72T"
      },
      "outputs": [],
      "source": [
        "# Load T5 Question Generation Model\n",
        "t5_qg_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "t5_qg_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrZXpGeevoND"
      },
      "source": [
        "### **Load Pre-Trained T5 for Question Generation**\n",
        "We use a **pre-trained `t5-small` model** to generate questions from text.\n",
        "\n",
        "- **Model Used:** `t5-small` (or `t5-base` for better accuracy)\n",
        "- **Tokenizer:** `T5Tokenizer`\n",
        "- **Task:** `\"generate questions: <passage>\" â†’ <question>\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD27KsVrCnZJ"
      },
      "outputs": [],
      "source": [
        "def generate_questions_t5(passage, num_questions=3):\n",
        "    \"\"\"Generates questions from a passage using T5.\"\"\"\n",
        "    input_text = \"generate questions: \" + passage\n",
        "    inputs = t5_qg_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        question_ids = t5_qg_model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=50,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    questions = t5_qg_tokenizer.decode(question_ids[0], skip_special_tokens=True)\n",
        "    return questions.split(\"\\n\")[:num_questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RvhvhuvD5Ba"
      },
      "outputs": [],
      "source": [
        "# Select More Passages for Question Generation\n",
        "sample_passages = passage_df[\"text\"].sample(n=100, random_state=42).tolist()  # Pick 100 random passages\n",
        "generated_questions = []\n",
        "\n",
        "for passage in sample_passages:\n",
        "    questions = generate_questions_t5(passage, num_questions=5)  # Generate 5 questions per passage\n",
        "    generated_questions.extend(questions)\n",
        "\n",
        "print(f\"Generated {len(generated_questions)} training questions!\")\n",
        "\n",
        "# Save the questions\n",
        "with open(\"generated_training_questions.txt\", \"w\") as f:\n",
        "    for q in generated_questions:\n",
        "        f.write(q + \"\\n\")\n",
        "\n",
        "print(\"Questions saved in 'generated_training_questions.txt'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1k3N5k-wMVx"
      },
      "source": [
        "# Fine-Tuning DPR with Generated Questions\n",
        "\n",
        "To improve **retrieval accuracy**, we fine-tune the **DPR Question Encoder** using the **auto-generated question-passage pairs**.\n",
        "\n",
        "### **Why Fine-Tune DPR?**\n",
        "- **Pre-trained DPR** was trained on general knowledge datasets.\n",
        "- **Fine-tuning with our dataset** helps it **better retrieve relevant book passages**.\n",
        "- Improves chatbot response quality by ensuring **accurate passage retrieval**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH-pWP3pFyai"
      },
      "outputs": [],
      "source": [
        "# Load generated questions\n",
        "with open(\"generated_training_questions.txt\", \"r\") as f:\n",
        "    training_questions = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Loaded {len(training_questions)} training questions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVY5zARbwgYN"
      },
      "source": [
        "To fine-tune DPR, we need question-passage pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EszNCtMSF3Il"
      },
      "outputs": [],
      "source": [
        "# Create DPR fine-tuning dataset\n",
        "dpr_training_data = []\n",
        "for question in training_questions:\n",
        "    retrieved_passages = retrieve_relevant_text(question, passage_df, top_k=1)  # Get best matching passage\n",
        "    best_passage = retrieved_passages[0]  # Best passage\n",
        "\n",
        "    dpr_training_data.append({\n",
        "        \"question\": question,\n",
        "        \"positive_passages\": [best_passage]  # Positive passage\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JiEU8WuF5vB"
      },
      "outputs": [],
      "source": [
        "# Save training data as JSON\n",
        "with open(\"dpr_training_data.json\", \"w\") as f:\n",
        "    json.dump(dpr_training_data, f, indent=4)\n",
        "\n",
        "print(f\"DPR Training Data Saved! {len(dpr_training_data)} question-passage pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-a_2OVEwkM3"
      },
      "source": [
        "DPR requires separate tokenization for questions and passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7GtKBPnZd7W"
      },
      "outputs": [],
      "source": [
        "def tokenize_dpr_data(examples):\n",
        "    \"\"\"Tokenizes both questions and passages for DPR fine-tuning.\"\"\"\n",
        "    # Tokenize questions using the question encoder tokenizer\n",
        "    question_encodings = q_tokenizer(\n",
        "        examples[\"question\"], truncation=True, padding=\"max_length\", max_length=512\n",
        "    )\n",
        "\n",
        "    # Tokenize passages using the context encoder tokenizer\n",
        "    passage_encodings = ctx_tokenizer(\n",
        "        examples[\"positive_passages\"], truncation=True, padding=\"max_length\", max_length=512\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": question_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": question_encodings[\"attention_mask\"],\n",
        "        \"input_ids_passage\": passage_encodings[\"input_ids\"],\n",
        "        \"attention_mask_passage\": passage_encodings[\"attention_mask\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MI6ZatVEqYe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset and apply tokenization\n",
        "def load_dpr_training_data():\n",
        "    \"\"\"Loads, tokenizes, and splits DPR training data into train & eval sets.\"\"\"\n",
        "    with open(\"dpr_training_data.json\", \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    questions = [item[\"question\"] for item in data]\n",
        "    passages = [item[\"positive_passages\"][0] for item in data]\n",
        "\n",
        "    # Split data (80% Train, 20% Eval)\n",
        "    train_qs, eval_qs, train_ps, eval_ps = train_test_split(questions, passages, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert to Hugging Face Dataset format\n",
        "    train_dataset = Dataset.from_dict({\"question\": train_qs, \"positive_passages\": train_ps})\n",
        "    eval_dataset = Dataset.from_dict({\"question\": eval_qs, \"positive_passages\": eval_ps})\n",
        "\n",
        "    # Apply Tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_dpr_data, batched=True, remove_columns=[\"question\", \"positive_passages\"])\n",
        "    eval_dataset = eval_dataset.map(tokenize_dpr_data, batched=True, remove_columns=[\"question\", \"positive_passages\"])\n",
        "\n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "# Get tokenized train & eval datasets\n",
        "dpr_train_dataset, dpr_eval_dataset = load_dpr_training_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g6IbPmrTNmo"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, AutoModelForSeq2SeqLM\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w5e9SsDwqPw"
      },
      "source": [
        "DPR uses contrastive learning, so we define a custom loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HvBdBGiaUB7"
      },
      "outputs": [],
      "source": [
        "class DPRTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer for DPR that computes loss manually.\"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"Computes contrastive loss for DPR Question & Passage encoders.\"\"\"\n",
        "\n",
        "        # Extract inputs for DPR\n",
        "        question_inputs = {\n",
        "            \"input_ids\": inputs[\"input_ids\"].to(model.device),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].to(model.device)\n",
        "        }\n",
        "        passage_inputs = {\n",
        "            \"input_ids\": inputs[\"input_ids_passage\"].to(model.device),\n",
        "            \"attention_mask\": inputs[\"attention_mask_passage\"].to(model.device)\n",
        "        }\n",
        "\n",
        "        # Get embeddings from DPR encoders\n",
        "        question_embeds = model(**question_inputs).pooler_output  # Shape: (batch_size, 768)\n",
        "        passage_embeds = model(**passage_inputs).pooler_output  # Shape: (batch_size, 768)\n",
        "\n",
        "        # Compute similarity (dot product)\n",
        "        similarity_scores = torch.matmul(question_embeds, passage_embeds.T)  # Shape: (batch_size, batch_size)\n",
        "\n",
        "        # Create labels (each question should match its corresponding passage)\n",
        "        batch_size = question_embeds.size(0)\n",
        "        labels = torch.arange(batch_size, device=model.device)\n",
        "\n",
        "        # Compute contrastive loss (CrossEntropy over similarity scores)\n",
        "        loss = F.cross_entropy(similarity_scores, labels)\n",
        "\n",
        "        return (loss, {\"loss\": loss}) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAsn8cznwt-A"
      },
      "source": [
        "We now train the DPR model using our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTuIz1fUCXW3"
      },
      "outputs": [],
      "source": [
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_dpr\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False  # Prevents Hugging Face from deleting tokenized inputs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asA3RPJ5Ca2w"
      },
      "outputs": [],
      "source": [
        "trainer = DPRTrainer(\n",
        "    model=q_encoder,  # Fine-tune the question encoder\n",
        "    args=training_args,\n",
        "    train_dataset=dpr_train_dataset,\n",
        "    eval_dataset=dpr_eval_dataset,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=q_tokenizer),\n",
        ")\n",
        "\n",
        "print(\"Starting DPR Fine-Tuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "q_encoder.save_pretrained(\"./fine_tuned_dpr\")\n",
        "print(\"Fine-Tuned DPR Model Saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFzWhHfTxD7o"
      },
      "source": [
        "# Fine-Tuning BERT-QA & Evaluating DPR\n",
        "\n",
        "Now that we have **fine-tuned the DPR retriever**, we move on to **evaluating retrieval accuracy** and **fine-tuning BERT-QA** for answer extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuXtocwYChp9"
      },
      "outputs": [],
      "source": [
        "# Load Fine-Tuned Model\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"./fine_tuned_dpr\").to(device)\n",
        "\n",
        "def encode_query(query):\n",
        "    \"\"\"Encodes the query using the fine-tuned DPR Question Encoder.\"\"\"\n",
        "    inputs = q_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        embedding = q_encoder(**inputs).pooler_output.cpu().numpy()  # Use fine-tuned model\n",
        "    return embedding\n",
        "\n",
        "print(\"Fine-Tuned DPR Model Loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16jYoAhhxLGR"
      },
      "source": [
        "We compare retrieval results before & after fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-ZR8Mw7DExK"
      },
      "outputs": [],
      "source": [
        "def test_retrieval_accuracy(queries, passage_df, fine_tuned=True):\n",
        "    \"\"\"Test retrieval performance before and after fine-tuning.\"\"\"\n",
        "    print(\"\\n=== Testing DPR Retrieval Accuracy ===\")\n",
        "\n",
        "    for query in queries:\n",
        "        retrieved_passages = retrieve_relevant_text(query, passage_df, top_k=1)\n",
        "        best_passage = retrieved_passages[0]\n",
        "\n",
        "        print(f\"\\n**Query:** {query}\")\n",
        "        print(f\"**Retrieved Passage:** {best_passage[:200]}...\")  # Show only first 200 chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrUHPX8FDKiF"
      },
      "outputs": [],
      "source": [
        "# Test Queries\n",
        "test_queries = [\n",
        "    \"What is crop rotation?\",\n",
        "    \"Explain the benefits of irrigation.\",\n",
        "    \"Describe sustainable farming methods.\",\n",
        "    \"How does composting improve soil?\",\n",
        "    \"What is the best way to maintain soil fertility?\",\n",
        "]\n",
        "\n",
        "# Test retrieval before fine-tuning (Use old model)\n",
        "print(\"\\n**Before Fine-Tuning:**\")\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "test_retrieval_accuracy(test_queries, passage_df, fine_tuned=False)\n",
        "\n",
        "# Test retrieval after fine-tuning (Use fine-tuned model)\n",
        "print(\"\\n**After Fine-Tuning:**\")\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"./fine_tuned_dpr\").to(device)\n",
        "test_retrieval_accuracy(test_queries, passage_df, fine_tuned=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5flx7zsqxRCw"
      },
      "source": [
        "Once we have high-quality retrieved passages, we need to fine-tune a BERT-QA model to extract accurate answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLFxqxJRDQlx"
      },
      "outputs": [],
      "source": [
        "# Load a moderate-sized BERT-QA model\n",
        "qa_model_name = \"deepset/bert-base-cased-squad2\"\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zklbvHmELlW"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, passages):\n",
        "    \"\"\"Extracts the best answer from multiple retrieved passages using BERT-QA.\"\"\"\n",
        "    best_answer = None\n",
        "    best_confidence = 0\n",
        "\n",
        "    for passage in passages:\n",
        "        inputs = qa_tokenizer(question, passage, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = qa_model(**inputs)\n",
        "\n",
        "        start_probs = torch.nn.functional.softmax(outputs.start_logits, dim=-1)\n",
        "        end_probs = torch.nn.functional.softmax(outputs.end_logits, dim=-1)\n",
        "\n",
        "        start_idx = torch.argmax(start_probs)\n",
        "        end_idx = torch.argmax(end_probs) + 1\n",
        "\n",
        "        answer_confidence = (start_probs[0][start_idx] + end_probs[0][end_idx - 1]) / 2\n",
        "\n",
        "        # âœ… Debug: Print confidence scores for each passage\n",
        "        print(f\"\\nðŸ” QA Debug: Confidence Score = {answer_confidence.item():.4f}\")\n",
        "\n",
        "        if answer_confidence > best_confidence:\n",
        "            best_confidence = answer_confidence\n",
        "            best_answer = qa_tokenizer.convert_tokens_to_string(\n",
        "                qa_tokenizer.convert_ids_to_tokens(inputs.input_ids[0][start_idx:end_idx])\n",
        "            ).strip()\n",
        "\n",
        "    # âœ… Handle cases where no confident answer was found\n",
        "    if not best_answer or best_answer in [\"[CLS]\", \"[SEP]\"]:\n",
        "        return \"I couldn't find a clear answer in the retrieved passages.\"\n",
        "\n",
        "    return best_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqUWnJSwxXse"
      },
      "source": [
        "We now use DPR to retrieve passages and BERT-QA to extract answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa9gn5mtENRP"
      },
      "outputs": [],
      "source": [
        "# Generate QA training data\n",
        "qa_training_data = []\n",
        "for question in training_questions:\n",
        "    retrieved_passages = retrieve_relevant_text(question, passage_df, top_k=1)  # Use fine-tuned DPR\n",
        "    best_passage = retrieved_passages[0]\n",
        "\n",
        "    answer = answer_question(question, best_passage)  # Extract answer using BERT-QA\n",
        "\n",
        "    if answer:  # Only keep high-confidence answers\n",
        "        qa_training_data.append({\n",
        "            \"context\": best_passage,\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"answer_start\": best_passage.find(answer)  # Locate answer position in text\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leIPJ45tEP4s"
      },
      "outputs": [],
      "source": [
        "# Save dataset\n",
        "with open(\"qa_training_data.json\", \"w\") as f:\n",
        "    json.dump(qa_training_data, f, indent=4)\n",
        "\n",
        "print(f\"QA Training Data Saved! {len(qa_training_data)} samples created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7boZqgKthiTd"
      },
      "outputs": [],
      "source": [
        "def tokenize_qa_data(examples):\n",
        "    \"\"\"Tokenizes context and question, and aligns answer spans safely.\"\"\"\n",
        "    encodings = qa_tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, answer in enumerate(examples[\"answer\"]):\n",
        "        answer_text = answer.strip()\n",
        "        answer_start = examples[\"answer_start\"][i]  # Original character index in the raw context\n",
        "\n",
        "        # Get tokenized context text\n",
        "        tokenized_context = qa_tokenizer.convert_ids_to_tokens(encodings[\"input_ids\"][i])\n",
        "\n",
        "        # Try to find the answer inside the tokenized text\n",
        "        answer_tokens = qa_tokenizer.tokenize(answer_text)\n",
        "        answer_length = len(answer_tokens)\n",
        "\n",
        "        # Try finding the first occurrence of answer tokens inside the tokenized context\n",
        "        for token_start in range(len(tokenized_context) - answer_length):\n",
        "            if tokenized_context[token_start: token_start + answer_length] == answer_tokens:\n",
        "                start_positions.append(token_start)\n",
        "                end_positions.append(token_start + answer_length - 1)\n",
        "                break\n",
        "        else:\n",
        "            # If answer was not found, assign safe defaults\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "\n",
        "    # Ensure valid integer indices\n",
        "    encodings[\"start_positions\"] = [max(0, pos) for pos in start_positions]\n",
        "    encodings[\"end_positions\"] = [max(start_positions[i], end_positions[i]) for i in range(len(start_positions))]\n",
        "\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNZdbL7RGFzX"
      },
      "outputs": [],
      "source": [
        "# Load dataset and apply tokenization\n",
        "def load_qa_training_data():\n",
        "    \"\"\"Loads, tokenizes, and prepares QA training data.\"\"\"\n",
        "    with open(\"qa_training_data.json\", \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"context\": [item[\"context\"] for item in data],\n",
        "        \"question\": [item[\"question\"] for item in data],\n",
        "        \"answer\": [item[\"answer\"] for item in data],\n",
        "        \"answer_start\": [item[\"answer_start\"] for item in data],\n",
        "    })\n",
        "\n",
        "    # Apply Safe Tokenization\n",
        "    tokenized_dataset = dataset.map(tokenize_qa_data, batched=True, remove_columns=[\"context\", \"question\", \"answer\", \"answer_start\"])\n",
        "\n",
        "    return tokenized_dataset\n",
        "\n",
        "# Get the safely tokenized dataset\n",
        "qa_train_dataset = load_qa_training_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwpHhFAgxfsE"
      },
      "source": [
        "Fine-Tune BERT-QA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qGjxVFQGS7c"
      },
      "outputs": [],
      "source": [
        "# Define Training Arguments\n",
        "qa_training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_bert_qa\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False  # Prevents Hugging Face from removing necessary columns\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ5bigNoGUxu"
      },
      "outputs": [],
      "source": [
        "# Initialize Trainer\n",
        "qa_trainer = Trainer(\n",
        "    model=qa_model,\n",
        "    args=qa_training_args,\n",
        "    train_dataset=qa_train_dataset,\n",
        ")\n",
        "\n",
        "print(\"Starting BERT-QA Fine-Tuning...\")\n",
        "qa_trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "qa_model.save_pretrained(\"./fine_tuned_bert_qa\")\n",
        "print(\"Fine-Tuned BERT-QA Model Saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ8NQTWxupU"
      },
      "source": [
        "# Fine-Tuning T5 for Summarization\n",
        "\n",
        "To allow the chatbot to generate **concise summaries** of book passages, we fine-tune **T5 (Text-To-Text Transfer Transformer)** for summarization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPufmCkVGaml"
      },
      "outputs": [],
      "source": [
        "# Load T5 Model & Tokenizer\n",
        "t5_model_name = \"t5-small\"  # You can also use \"t5-base\" for better quality\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFGbD_86x22A"
      },
      "source": [
        "Before fine-tuning, we generate summaries using the pre-trained T5 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5AG9_z7G3bh"
      },
      "outputs": [],
      "source": [
        "def generate_summary(passage):\n",
        "    \"\"\"Generates a summary from a passage using T5.\"\"\"\n",
        "    input_text = \"summarize: \" + passage\n",
        "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = t5_model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=150,\n",
        "            num_beams=5,\n",
        "            length_penalty=0.7,  # Encourage concise summaries\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th8bJapEx7AY"
      },
      "source": [
        "We sample 100 random passages from the books and generate summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJvRpeFuG5P8"
      },
      "outputs": [],
      "source": [
        "# Generate Training Data for Summarization\n",
        "t5_training_data = []\n",
        "sample_passages = passage_df[\"text\"].sample(n=500, random_state=42).tolist()  # Pick 100 random passages\n",
        "\n",
        "for passage in sample_passages:\n",
        "    summary = generate_summary(passage)  # Generate summary with pretrained T5\n",
        "    t5_training_data.append({\n",
        "        \"text\": passage,\n",
        "        \"summary\": summary\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ0aEhMLG8TO"
      },
      "outputs": [],
      "source": [
        "# Save dataset\n",
        "with open(\"t5_training_data.json\", \"w\") as f:\n",
        "    json.dump(t5_training_data, f, indent=4)\n",
        "\n",
        "print(f\"Summarization Training Data Saved! {len(t5_training_data)} samples created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV54IlXcyVZ1"
      },
      "source": [
        "We load the dataset and apply tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0daHlxZ0fR02"
      },
      "outputs": [],
      "source": [
        "def tokenize_t5_data(examples):\n",
        "    \"\"\"Tokenizes input text and summary for T5 fine-tuning.\"\"\"\n",
        "    model_inputs = t5_tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    labels = t5_tokenizer(\n",
        "        examples[\"summary\"],\n",
        "        max_length=150,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMTzlAZXHAD6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load dataset and apply tokenization\n",
        "def load_t5_training_data():\n",
        "    \"\"\"Loads and tokenizes T5 training data.\"\"\"\n",
        "    with open(\"t5_training_data.json\", \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"text\": [item[\"text\"] for item in data],\n",
        "        \"summary\": [item[\"summary\"] for item in data]\n",
        "    })\n",
        "\n",
        "    # Apply Tokenization\n",
        "    tokenized_dataset = dataset.map(tokenize_t5_data, batched=True, remove_columns=[\"text\", \"summary\"])\n",
        "\n",
        "    return tokenized_dataset\n",
        "\n",
        "# Get tokenized training dataset\n",
        "t5_train_dataset = load_t5_training_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvzN4l99yPPF"
      },
      "source": [
        "Once the dataset is ready, we train T5 on our book passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CoNRqSbHMS_"
      },
      "outputs": [],
      "source": [
        "# Define Training Arguments\n",
        "t5_training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_t5\",\n",
        "    evaluation_strategy=\"no\",  # No validation dataset\n",
        "    save_steps=500,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False  # Prevents Hugging Face from deleting tokenized inputs\n",
        ")\n",
        "\n",
        "# Fine-Tune T5\n",
        "t5_trainer = Trainer(\n",
        "    model=t5_model,\n",
        "    args=t5_training_args,\n",
        "    train_dataset=t5_train_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WdE0e8HHOwR"
      },
      "outputs": [],
      "source": [
        "print(\"Starting T5 Fine-Tuning...\")\n",
        "t5_trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "t5_model.save_pretrained(\"./fine_tuned_t5\")\n",
        "print(\"Fine-Tuned T5 Model Saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ij-Lv-myp6W"
      },
      "source": [
        "### Building the Conversational Chatbot\n",
        "\n",
        "Now that we have **fine-tuned DPR, BERT-QA, and T5**, we integrate them into a **Gradio-based chatbot** that can:\n",
        "-  **Summarize agricultural topics** using T5.\n",
        "-  **Answer questions** using BERT-QA.\n",
        "-  **Retrieve relevant book passages** using DPR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAXDzvl7TscI"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U87qkIipHRyE"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Load Fine-Tuned Models\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"./fine_tuned_dpr\").to(device)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"./fine_tuned_bert_qa\").to(device)\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"./fine_tuned_t5\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiCINUQ1yyXN"
      },
      "source": [
        "The chatbot detects user intent and selects the appropriate model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMcsN1BuH-Lu"
      },
      "outputs": [],
      "source": [
        "def chatbot_response(user_input):\n",
        "    \"\"\"Processes user input and returns either a QA answer or a summary.\"\"\"\n",
        "\n",
        "    # Detect user intent\n",
        "    if any(kw in user_input.lower() for kw in [\"tell me about\", \"describe\", \"summarize\", \"explain\"]):\n",
        "        task = \"summarization\"\n",
        "    elif user_input.strip().endswith(\"?\"):\n",
        "        task = \"qa\"\n",
        "    else:\n",
        "        return \"I'm not sure how to answer that. Try asking a question or requesting a summary.\"\n",
        "\n",
        "    # Retrieve the most relevant passage using fine-tuned DPR\n",
        "    retrieved_passages = retrieve_relevant_text(user_input, passage_df, top_k=1)\n",
        "    best_passage = retrieved_passages[0]\n",
        "\n",
        "    # Execute the correct task\n",
        "    if task == \"qa\":\n",
        "        response = answer_question(user_input, best_passage)\n",
        "        return f\"**Answer:** {response}\\n\\n**Source:** {best_passage}\"\n",
        "    elif task == \"summarization\":\n",
        "        response = generate_summary(best_passage)\n",
        "        return f\"**Summary:** {response}\\n\\n**Source:** {best_passage}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAs-hgjXy7DJ"
      },
      "source": [
        "We use Gradio to create an interactive chatbot interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8GkGbvXIA4s"
      },
      "outputs": [],
      "source": [
        "# Launch the chatbot\n",
        "gr.Interface(\n",
        "    fn=chatbot_response,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question or request a summary...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Conversational Agriculture Chatbot\",\n",
        "    description=\"Ask about agricultural topics. Try 'What is crop rotation?' or 'Tell me about soil fertility.'\",\n",
        "    live=True\n",
        ").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c8VjTLuy-Mm"
      },
      "source": [
        "We define a set of test cases to verify chatbot behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7XX1bUtIC7K"
      },
      "outputs": [],
      "source": [
        "test_cases = [\n",
        "    {\"input\": \"What is crop rotation?\", \"expected\": \"QA\"},\n",
        "    {\"input\": \"Tell me about soil fertility.\", \"expected\": \"Summarization\"},\n",
        "    {\"input\": \"Explain the benefits of irrigation.\", \"expected\": \"Summarization\"},\n",
        "    {\"input\": \"How does composting help soil?\", \"expected\": \"QA\"},\n",
        "    {\"input\": \"Describe sustainable farming.\", \"expected\": \"Summarization\"},\n",
        "    {\"input\": \"Who wrote this book?\", \"expected\": \"QA\"},\n",
        "    {\"input\": \"What is the capital of France?\", \"expected\": \"QA\"},\n",
        "    {\"input\": \"Summarize this book.\", \"expected\": \"Summarization\"},\n",
        "    {\"input\": \"How can I improve my soil quality?\", \"expected\": \"QA\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMvchuJcIcoO"
      },
      "outputs": [],
      "source": [
        "def test_chatbot():\n",
        "    \"\"\"Runs predefined test cases to evaluate chatbot performance.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    print(\"\\n=== Running Chatbot Tests ===\")\n",
        "\n",
        "    for i, case in enumerate(test_cases):\n",
        "        print(f\"\\n**Test Case {i+1}: {case['input']}**\")\n",
        "\n",
        "        response = chatbot_response(case[\"input\"])\n",
        "\n",
        "        # Detect output type\n",
        "        if \"**Answer:**\" in response:\n",
        "            detected = \"QA\"\n",
        "        elif \"**Summary:**\" in response:\n",
        "            detected = \"Summarization\"\n",
        "        else:\n",
        "            detected = \"Unknown\"\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            \"Test Case\": i + 1,\n",
        "            \"Input\": case[\"input\"],\n",
        "            \"Expected\": case[\"expected\"],\n",
        "            \"Detected\": detected,\n",
        "            \"Pass\": detected == case[\"expected\"],\n",
        "            \"Response\": response[:200]  # Show only first 200 chars\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"  - Expected: {case['expected']}\")\n",
        "        print(f\"  - Detected: {detected}\")\n",
        "        print(f\"  - Pass: {result['Pass']}\")\n",
        "        print(f\"  - Response Preview: {result['Response']}...\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSy2QrkqIfnI"
      },
      "outputs": [],
      "source": [
        "# Run the test cases\n",
        "test_results = test_chatbot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLiDOwE10lVi"
      },
      "source": [
        "# Reflection on the Chatbot's Performance\n",
        "\n",
        "This project aims to implement a multi-component NLP chatbot capable of retrieving passages, summarizing content, and answering questions using fine-tuned DPR, BERT-QA, and T5. However, upon further evaluation, a critical issue was identifiedâ€”the chatbot consistently returned [CLS] as the answer to all QA queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YstQJdgA0vCB"
      },
      "source": [
        "### Analysis of the [CLS] Response Issue\n",
        "The chatbot was expected to extract meaningful answers from book passages using BERT-QA. Instead, it returned [CLS] for every response, indicating that the model failed to extract proper answer spans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHCgGB-q00BB"
      },
      "source": [
        "### Potential Causes & Fixes\n",
        "1. Incorrect Token Alignment in BERT-QA\n",
        "2. Low Confidence in Answer Extraction\n",
        "3. Poor Passage Retrieval from DPR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nguvlje1Clw"
      },
      "source": [
        "# Final thoughts\n",
        "\n",
        "The chatbot successfully classifies queries but fails to provide valid QA responses. Fixing passage retrieval and token alignment should significantly improve performance. Once these adjustments are made, the chatbot will be better equipped to answer questions reliably rather than defaulting to [CLS]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
