{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going from raw text to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install tokenizers\n",
    "%pip install torch\n",
    "%pip install tqdm\n",
    "%pip install requests\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load book from Gutenberg (Pride and Prejudice) and tokenize the text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download the book\n",
    "# No need for a local file\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "print(\"Downloaded the book successfully!\")\n",
    "\n",
    "# Extract the main content\n",
    "text = response.text\n",
    "print(\"Raw text length:\", len(text))\n",
    "\n",
    "# Locate the true starting point\n",
    "start_index = text.find(\"It is a truth universally acknowledged\")\n",
    "end_index = text.rfind(\"had been the means of uniting them.\")\n",
    "clean_text = text[start_index:end_index].strip()\n",
    "\n",
    "# emove unwanted formatting using regex\n",
    "clean_text = re.sub(r\"Heading to\", \"\", clean_text)  # Remove 'Heading to'\n",
    "clean_text = re.sub(r\"\\[.*?\\]\", \"\", clean_text)  # Remove content inside square brackets\n",
    "clean_text = re.sub(r\"\\d+\", \"\", clean_text)  # Remove numbers\n",
    "clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()  # Normalize spaces\n",
    "\n",
    "print(\"Cleaned text length:\", len(clean_text))\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = nltk.sent_tokenize(clean_text)\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "\n",
    "# Display the first few sentences\n",
    "for i, sentence in enumerate(sentences[:5]):\n",
    "    print(f\"{i+1}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_words(text):\n",
    "    # Create an empty set to store unique words\n",
    "    unique_words = set()\n",
    "\n",
    "    # Split text into words using regular expressions\n",
    "    words = re.split(r'\\W+', text.lower())  # This splits at any non-alphanumeric character\n",
    "\n",
    "    # Add each word to the set\n",
    "    for word in words:\n",
    "        if word:  # This check avoids adding empty strings\n",
    "            unique_words.add(word)\n",
    "\n",
    "    # Return the number of distinct words\n",
    "    return len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pre-trained tokenizer from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the first 2 sentences as an example\n",
    "tokenized = [tokenizer.tokenize(sentence) for sentence in sentences[:2]]\n",
    "for i, tokens in enumerate(tokenized):\n",
    "    print(f\"Sentence {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Function to train WordLevel Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tokenize_word_level(sentences, vocab_size):\n",
    "    \"\"\"\n",
    "    Train and apply a word-level tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list): List of sentences to train the tokenizer on.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        \n",
    "    Returns:\n",
    "        tokenizer: The trained word-level tokenizer.\n",
    "        tokenized_sentences: Tokenized version of the input sentences.\n",
    "    \"\"\"\n",
    "    # Initialize a word-level tokenizer\n",
    "    tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "    # Set up pre-tokenization and trainer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\"])\n",
    "\n",
    "    # Train the tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    tokenized_sentences = [tokenizer.encode(sentence).tokens for sentence in sentences]\n",
    "\n",
    "    return tokenizer, tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function to train subword Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tokenize_subword(sentences, vocab_size):\n",
    "    \"\"\"\n",
    "    Train and apply a subword-level tokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "        sentences (list): List of sentences to train the tokenizer on.\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        \n",
    "    Returns:\n",
    "        tokenizer: The trained subword-level tokenizer.\n",
    "        tokenized_sentences: Tokenized version of the input sentences.\n",
    "    \"\"\"\n",
    "    # Initialize a subword-level tokenizer (BPE model)\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "    # Set up pre-tokenization and trainer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\"])\n",
    "\n",
    "    # Train the tokenizer\n",
    "    tokenizer.train_from_iterator(sentences, trainer)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    tokenized_sentences = [tokenizer.encode(sentence).tokens for sentence in sentences]\n",
    "\n",
    "    return tokenizer, tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the amount of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = count_distinct_words(clean_text)\n",
    "\n",
    "print(f\"Total Unique Words: {num_unique_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with different vocabulary sizes in each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = [num_unique_words // 10, num_unique_words // 2, num_unique_words]\n",
    "\n",
    "print(\"\\nWord-Level Tokenizer Experiment:\")\n",
    "for vocab_size in vocab_sizes:\n",
    "    word_tokenizer, word_tokenized = train_and_tokenize_word_level(sentences, vocab_size)\n",
    "    print(f\"Vocab Size {vocab_size}: \\n{word_tokenized[0]}\\n{word_tokenized[1]}\\n\")\n",
    "\n",
    "print(\"Subword-Level Tokenizer Experiment:\")\n",
    "for vocab_size in vocab_sizes:\n",
    "    subword_tokenizer, subword_tokenized = train_and_tokenize_subword(sentences, vocab_size)\n",
    "    print(f\"Vocab Size {vocab_size}: \\n{subword_tokenized[0]}\\n{subword_tokenized[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test generating text using the trained tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, tokenized_sentences, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Generate text by reversing the tokenization process.\n",
    "\n",
    "    Parameters:\n",
    "        tokenizer: The tokenizer used for tokenization.\n",
    "        tokenized_sentences: List of tokenized sentences.\n",
    "        num_sentences: Number of sentences to generate text for.\n",
    "\n",
    "    Returns:\n",
    "        str: Reconstructed text from tokens.\n",
    "    \"\"\"\n",
    "    generated_text = []\n",
    "    for tokens in tokenized_sentences[:num_sentences]:\n",
    "        # Decode tokens to reconstruct the text\n",
    "        text = tokenizer.decode(tokenizer.encode(\" \".join(tokens)).ids)\n",
    "        generated_text.append(text)\n",
    "    \n",
    "    return \"\\n\".join(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for word-level tokenizer\n",
    "print(\"Generated Text (Word-Level, Vocab=100%):\")\n",
    "print(generate_text(word_tokenizer, word_tokenized, num_sentences=3))\n",
    "\n",
    "# Example usage for subword-level tokenizer\n",
    "print(\"\\nGenerated Text (Subword-Level, Vocab=100%):\")\n",
    "print(generate_text(subword_tokenizer, subword_tokenized, num_sentences=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
